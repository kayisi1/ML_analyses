{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F62B_4pcRI-"
      },
      "source": [
        "# Programming Assignment 5: Value Iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoKkTi4Gy7FF"
      },
      "source": [
        "## Collaboration statement\n",
        "\n",
        "List all collaborators by name, including other students and/or course staff (e.g. TAs, UCAs). If you collaborated with no one on this assignment, write the following: ``\"I did not collaborate with anyone on this assignment.\"``\n",
        "\n",
        "**Collaborators: Rajiv Swamy (UCA), Yongwei Che (UCA), Alfred Ripoll**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052JVvHvcbQ9"
      },
      "source": [
        "## Problem Setting\n",
        "\n",
        "**Gridworld**: In this assignment, we use the popular RL toolkit  [OpenAI Gym](https://gym.openai.com/)  to implement _value iteration_ on a gridworld environment (Lecture 21). In the gridworld environment we consider, an agent needs to navigate on a 2-D plane by using 4 actions (left, right, up, and down). It starts at a designated state (starting point \"S\"), and needs to try and reach a goal state (\"G\"). The agent controls the movement of a character in a grid world. Some tiles on the grid are walkable (\"S\", \"G\", and \"F\"), and others (\"H\") lead to the agent falling into the water, which ends the episode. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The agent stays in the same position if the transition would otherwise have put them out of bounds.\n",
        "\n",
        "**Rewards**: The environment is such that reaching the goal state gives the agent a `+1` reward, and it gets a `0` reward in other states (detailed description below). Hence, reaching the goal state is equivalent to maximizing the rewards it can accumulate.\n",
        "\n",
        "**Uncertainty**: The environment also has some uncertainty. That is, the movement direction of the agent is uncertain and only partially depends on the chosen direction. In our setting (detailed below), each tile in the grid is \"slippery\", which corresponds to the following uncertainty; if the agent chose to move up/down, it will be able to do so with probability `1/3`, but with probability `2/3` it will move right/left (`1/3` right, `1/3` left). Similarly for the case that the agent chose to move right/left, it might move up/down instead. However, you will always move by 1 tile only (i.e., no \"jumps\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBzucYDmz3rV"
      },
      "source": [
        "**Example**\n",
        "\n",
        "\n",
        "In a) the agent (penguin) tries to walk down, in b) it tries to walk right, and c) shows a potential pitfall where just by walking by a hole \"H\" (and not directly into it), the agent still has a `1/3` chance to fall into the hole, which ends the episode with no reward.\n",
        "\n",
        "\n",
        "Note that:\n",
        "* The specific transition probabilities don't matter much here, but it is important to keep in mind is that the action you choose to move towards only partially determined where you actually move, due to the uncertianty (\"slipperiness\") of the environment.\n",
        "\n",
        "* Recall that while you can see the whole gridworld (all the states), the agent only has information about its current state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19XeaxoQJiI"
      },
      "source": [
        "## Environment: Frozen Lake\n",
        "Winter is here. You and your friends are tossing around a frisbee at the park when you make a wild throw that leaves the frisbee stranded out in the middle of\n",
        "a lake. The water is mostly frozen (\"F\"), but there are a few holes (\"H\") where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend, and sometimes slip into another, adjacent tile! The surface is described using a grid like the following $8 \\times 8$ example:\n",
        "\n",
        "    SFFFFFFF\n",
        "    FFFFFFFF\n",
        "    FFFHFFFF\n",
        "    FFFFFHFF\n",
        "    FFFHFFFF\n",
        "    FHHFFFHF\n",
        "    FHFFHFHF\n",
        "    FFFHFFFG\n",
        "\n",
        "\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole \"H\". You receive a reward of 1 if you reach the goal, and 0 otherwise. The steps that you can make are one of the following: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3.\n",
        "\n",
        "## Visualization\n",
        "It is easier to understand how this environment behaves if you see it first.  \n",
        "\n",
        "**HINT:** We **strongly recommend** that before implementing anything, you start by first running all cells of this notebook, and scroll down to the cell under **ACT7** see how a **totally-random** agent moves around the frozen lake.\n",
        "\n",
        "You will see it fail many times. In what follows, we will implement a much better agent that can navigate to the goal. **You can use the visualization to see how your agent performs, where it fails, and debug issues that arise during development.**\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCQ3b6BQQJiI"
      },
      "source": [
        "## Notation\n",
        "\n",
        "Mapping of the OpenAI gym constructs to the notation used in lectures:\n",
        "* The value function $V(s)$, is implemented here as a small array, of size corrsponsing to number of states, such that `V[s]` contains the value $V(s)$.\n",
        "* The transition probabilities of the environment $P(s'|s,a)$ correspond to the gym object `env.P` described below.\n",
        "* In lectures, the reward function $r(a|s, s')$ signified  taking action $a$ while in state $s$ moving to state $s'$. However, in our setting here the reward fixed into the environment is such that $r(a|s=\\text{'G'}, s')=1$ and $r(a|s \\neq\\text{'G'}, s')=0$ for any $a, s'$. In other words, in this setting we only get the reward if we *actually reach* the goal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "### First, install dependencies (takes ~1 min)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "outputs": [],
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay pygame > /dev/null 2>&1\n",
        "!apt-get install -y xvfb libav-tools python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import pygame\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyVgSS0Qlx_",
        "outputId": "5d685949-f5d5-4255-b9d3-4f6e67dc2096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# setting up dummy display driver for Colab\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "pygame.display.set_mode((640,480))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "outputs": [],
      "source": [
        "# first, we set some useful parameters:\n",
        "gamma = 0.99        # discount factor\n",
        "theta = 0.000001    # precision of value_iteration\n",
        "\n",
        "# let's set up the frozen lake environment.\n",
        "env = gym.make(\"FrozenLake8x8-v1\", new_step_api=True) # create the environment\n",
        "env = env.unwrapped # unwrap it to have additional information from it\n",
        "\n",
        "# spaces dimension\n",
        "num_actions = env.action_space.n # we can move up/down/left/right, i.e., 4 actions.\n",
        "num_states = env.observation_space.n # we have one state per tile in the grid (64 states)\n",
        "\n",
        "# NOTE, the transition probabilities we mentioned above (i.e., the \"slipperiness\"), are already given\n",
        "# in the env.P object. Specifically, env.P are the transition probabilities (dictionary dict of dicts of lists)\n",
        "#           P[s][a] == [(probability, s', reward, done),\n",
        "#                       (probability, s', reward, done),\n",
        "#                       (probability, s', reward, done), ]\n",
        "\n",
        "# As an example, for state s=0 (the starting tile \"S\"),and action a=0 (LEFT), you will have:\n",
        "#           P[0][0] == [(1/3, 0, 0, False), (1/3, 0, 0, False), (1/3, 4, 0, False)]\n",
        "# since:\n",
        "# with probability 1/3 you'll move UP (but there's no up, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move LEFT (but there's no left, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move DOWN (so you'll get to the tile below, which is s'=8).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDP6Ff27k_uk"
      },
      "source": [
        "## Value-Iteration algorithm\n",
        "\n",
        "We will now implement the Value Iteration algorithm you have seen in class. The pseudo-code is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPLXehxpQJiL"
      },
      "source": [
        "### Pseudo-code\n",
        "1. Parameter: a small threshold $\\theta>0$ determining accuracy of estimation.\n",
        "1. Initialize $V(s)=-\\frac{M}{1-\\gamma}$, for all $s\\in\\mathbb{S}$, where $M$ is the upper bound on the absolute value of immediate rewards.\n",
        "1. **Loop**:\n",
        "    1. $\\Delta \\leftarrow 0$.\n",
        "    1. **Loop for each** $s\\in\\mathbb{S}$:\n",
        "        1. $v\\leftarrow V(s)$.\n",
        "        1. $V(s) \\leftarrow \\max_a\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr]$.\n",
        "        1. $\\Delta\\leftarrow \\max(\\Delta, \\lvert v - V(s) \\rvert)$.\n",
        "1. **until** $\\Delta < \\theta$.\n",
        "1. Output a deterministic policy, $\\pi\\approx\\pi_*$, such that\n",
        "$$ \\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr].$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZrrQafMQJiL"
      },
      "source": [
        "## ACTS 1-2: Computing the sum & argmax of Value-Iteration.\n",
        "\n",
        "Compute the inner sum $$\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr],$$ given $V, s, a, \\gamma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5topu9WrLNo_"
      },
      "outputs": [],
      "source": [
        "# ACT 1: compute the inner sum of the equations above, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, Action a, and discount factor gamma.\n",
        "        # Arguments (V, s, a, gamma) should not be mutated in this ACT\n",
        "\n",
        "def sum_over_states(V, s, a, gamma):\n",
        "    sum_val = 0\n",
        "    # recall that env.P is a list of tuples for (s,a): [(prob, s', r, done),...] (See block above for more details!)\n",
        "    # where 'prob' is the transition probability P(s'|s,a).\n",
        "    # also, since r is associated with the states, you can think of the sum as only traversing states s'.\n",
        "    # therefore, all is needed is to iterate over each possible transition to state s'\n",
        "    # from the pair (s,a), and compute the sum value (as in the pseudo-code).\n",
        "    ### YOUR CODE GOES HERE\n",
        "    num = 0\n",
        "    for step in env.P[s][a]:\n",
        "      prob = env.P[s][a][num][0]\n",
        "      reward = env.P[s][a][num][2]\n",
        "      vals = V[env.P[s][a][num][1]]\n",
        "      sum = prob * ((reward) + (gamma*vals))\n",
        "      sum_val += sum\n",
        "      num += 1\n",
        "    return sum_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzRc5WrWQJiM"
      },
      "source": [
        "We now compute the argmax operation, in which we will update $\\pi(a|s)$ where $s$ is the given state, and $a$ is the maximizing action! To obtain it, you should use the function above `sum_over_states`.\n",
        "\n",
        "**HINT**:\n",
        "In the scaffold code for ACT5-6, pi is initialized to a zero matrix. If all the ACTs are implemented correctly, then the optimal policy pi returned by the `value_iteration` function (from ACT5-6) should have the following property: each row in pi should contain all zeros except at one index, where it contains a 1 (i.e. the index corresponding to the maximizing action for the state corresponding to that row).\n",
        "\n",
        "Suppose we had a simple environment with 2 states and 3 actions. Then, our pi would be a 2 by 3 matrix. Suppose the following pi matrix was returned by `value_iteration`:\n",
        "\n",
        "$$\n",
        "\\pi =\n",
        "\\begin{bmatrix}\n",
        "  1 & 0 & 0 \\\\\n",
        "  0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This would mean that when we're in state 0 (i.e. row index 0), we should take action 0 (i.e. column index 0 contains a 1) and when we're in state 1 (i.e. row index 1), we should take action 2 (i.e. column index 2 contains a 1).\n",
        "\n",
        "Now going back to `argmax_over_actions` in ACT2, Given state s, suppose action a maximizes the inner sum. How should pi be updated? (You can assume pi[s] contains all zeros, from the scaffold code in ACT5-6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "b7WLWpUMQJiM"
      },
      "outputs": [],
      "source": [
        "# ACT 2: update the action which can take us to a higher valued state, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # Policy array pi (a matrix of shape [num_states, num_actions], you can assume it contains all zero entries),\n",
        "        # State s, and discount factor gamma.\n",
        "        # Return the updated pi matrix and max_val (i.e. the value of taking the maximizing action from state s)\n",
        "\n",
        "def argmax_over_actions(V, pi, s, gamma):\n",
        "    max_val = float('-inf')\n",
        "    max_action = 0\n",
        "    for action in range(num_actions):\n",
        "      new_val = sum_over_states(V, s, action, gamma)\n",
        "      if new_val > max_val:\n",
        "        max_action = action\n",
        "        max_val = new_val\n",
        "    pi[s, max_action] = 1\n",
        "    return pi, max_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXHKhMPQJiM"
      },
      "source": [
        "## ACT 3: Computing the Bellman update\n",
        "\n",
        "We now perform a single iteration of update to the value function $V$, given state $s$, and discount factor $\\gamma$.\n",
        "You may call a function you have implemented above, and output the difference between the new, updated value for this state, and the previous value (denoted as `delta` in the pseudo-code above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "_TADCWSLfXB4"
      },
      "outputs": [],
      "source": [
        "# ACT 3: update the Value function for state s, that is: V[s], by taking action which maximizes current value. Given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, and discount factor gamma.\n",
        "# V should be updated in this ACT\n",
        "\n",
        "def bellman_optimality_update(V, s, gamma):\n",
        "    pi = np.zeros((num_states, num_actions))\n",
        "    delta = 0\n",
        "    prev_val = V[s]\n",
        "    ### YOUR CODE GOES HERE\n",
        "    # ACT3a: call a function implemented above, to get the action which maximizes current value.\n",
        "    bell_pi, new_maxval = argmax_over_actions(V, pi, s, gamma)\n",
        "        # ACT3b: update value V[s]\n",
        "    V[s] = new_maxval\n",
        "        # ACT 3c: compute and output delta.\n",
        "    delta = abs(prev_val - V[s])\n",
        "    return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2xStxIWQJiM"
      },
      "source": [
        "## ACT 4: Initialize V\n",
        "Initialize $V$ such that for all states, $V[s] = -\\frac{M}{1-\\gamma}$, where $M$ is the upper bound on the absolute value of immediate rewards to be filled in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "tMmwrAeWQJiN"
      },
      "outputs": [],
      "source": [
        "# ACT 4: Initialize V\n",
        "      # length_V gives the size of the vector V\n",
        "def init_V(gamma, M, length_V):\n",
        "    ### YOUR CODE GOES HERE\n",
        "    V = np.zeros(length_V)\n",
        "    for state in range(num_states):\n",
        "      V[state] = -1 * (M / (1-gamma))\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gXAhs3cQJiN"
      },
      "source": [
        "## ACT 5-6: Value-iteration (putting it all together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "LsGJ0nSieiix"
      },
      "outputs": [],
      "source": [
        "# Now, let's put it all together. Recall that we are given:\n",
        "        # discount factor gamma, and wanted precision theta.\n",
        "def value_iteration(gamma, theta):\n",
        "    # Initialize V with init_V function\n",
        "    ### YOUR CODE GOES HERE\n",
        "    M = 1\n",
        "    V = init_V(gamma, M, num_states)\n",
        "    # ACT 5: construct the main loop,\n",
        "    #        iteratively calling the bellman update,\n",
        "    #        and break when sufficient accuracy was reached.\n",
        "    while True:\n",
        "        delta = 0\n",
        "        ### YOUR CODE GOES HERE\n",
        "        for state in range(num_states):\n",
        "          delta1 = bellman_optimality_update(V, state, gamma)\n",
        "          maxdelta = max(delta, delta1)\n",
        "        if maxdelta < theta:\n",
        "          break\n",
        "    pi = np.zeros((num_states, num_actions))\n",
        "    # ACT 6: Extract optimal policy using the\n",
        "    # argmax_over_actions function defined in ACT 2\n",
        "    ### YOUR CODE GOES HERE\n",
        "    for state in range(num_states):\n",
        "      pi, V2 = argmax_over_actions(V, pi, state, gamma)\n",
        "    # output optimal value funtion, optimal policy\n",
        "    return V, pi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MvIfGO3QJiN"
      },
      "source": [
        "## Helper functions (**no action needed**)\n",
        "The next two functions `show_state` and `pretty_print` are helper functions we provide that print and visualize state information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "WaSJKClYRpAw"
      },
      "outputs": [],
      "source": [
        "def show_state(env, trial=0, step=0):\n",
        "    plt.figure(5)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(f\"Trial: {trial} | Step: {step}\")\n",
        "    plt.axis('off')\n",
        "    clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Y2GuhYhn4Gtb"
      },
      "outputs": [],
      "source": [
        "def pretty_print(i_episode, t, done, reward, total_rewards, debug=False):\n",
        "  if debug:\n",
        "    show_state(env, i_episode, t)\n",
        "    if done:\n",
        "        print('')\n",
        "        if reward == 1:\n",
        "            total_rewards += 1\n",
        "            print(\">> Success!! got the goal\")\n",
        "        else:\n",
        "            print(\">> Failed!! fell into a hole\")\n",
        "        time.sleep(3), clear_output(wait=True)\n",
        "    else:\n",
        "      time.sleep(.5)\n",
        "    it = i_episode + 1\n",
        "    print(f\"Avg reward so far = {(total_rewards / it):.2f}\")\n",
        "  else:\n",
        "    if done:\n",
        "      if reward == 1:\n",
        "        total_rewards += 1\n",
        "      it = i_episode + 1\n",
        "      if it%10 == 0:\n",
        "        print(f\"Avg reward obtained in iteration {it} = {(total_rewards / it):.2f}\")\n",
        "  return total_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZpAonqnQJiN"
      },
      "source": [
        "## ACT 7: Call agent, and evaluate results\n",
        "\n",
        "Below is the main loop of our setting: first, we call the `value_iteration` function, and obtain the optimal policy.\n",
        "Then, we run in many episodes (or trials) of the algorithm. In each trial, we start at the starting state \"S\". We then ask the agent what action to go towards. We make a step in the frozen lake with the chosen action (but may not move there because the lake is slippery!), our next state is given after we call `env.step(action)`.\n",
        "\n",
        "*You can expect to get an average reward in the range (0.8,1) upon correct implementation.*\n",
        "\n",
        "**Note**: While running over 100 episodes, set `debug = False` to speed up your execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nj5sjsk15IT",
        "outputId": "fb750d2e-e642-4d1e-9558-6e19e7a06c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg reward obtained in iteration 10 = 0.80\n",
            "Avg reward obtained in iteration 20 = 0.90\n",
            "Avg reward obtained in iteration 30 = 0.87\n",
            "Avg reward obtained in iteration 40 = 0.82\n",
            "Avg reward obtained in iteration 50 = 0.80\n",
            "Avg reward obtained in iteration 60 = 0.80\n",
            "Avg reward obtained in iteration 70 = 0.81\n",
            "Avg reward obtained in iteration 80 = 0.82\n",
            "Avg reward obtained in iteration 90 = 0.82\n",
            "Avg reward obtained in iteration 100 = 0.81\n"
          ]
        }
      ],
      "source": [
        "# note that you can first try it out with a random agent, by setting this variable to True.\n",
        "random_agent = False\n",
        "episodes = 100\n",
        "\n",
        "# When `debug` is set to True, you can see visualizations of the agent's actions\n",
        "debug = False\n",
        "\n",
        "if not random_agent:\n",
        "    # We have called the value_iteration function:\n",
        "    V_, pi = value_iteration(gamma, theta)\n",
        "    # note that we don't need the value function V anymore, as we already extracted the optimal policy!\n",
        "\n",
        "total_rewards = 0\n",
        "for i_episode in range(episodes):\n",
        "      state = env.reset()\n",
        "      t = 0\n",
        "      while True:\n",
        "          t+=1\n",
        "          # your agent picks an action:\n",
        "          if random_agent:\n",
        "            action = env.action_space.sample() # totally random action !\n",
        "          else:\n",
        "            # ACT 7: get the best action according to optimal policy `pi`, and current state `state`.\n",
        "            action = 0\n",
        "            ### YOUR CODE GOES HERE\n",
        "            action = np.argwhere(pi[state])[0][0]\n",
        "          # make a step according to policy\n",
        "          state, reward, terminated, truncated, info = env.step(action)\n",
        "          # the above variables are:\n",
        "                # state (object): agent's observation, current state (new state we've reached after the step we took)\n",
        "                # reward (float) : amount of reward returned after previous action\n",
        "                # done (bool): whether the episode has ended\n",
        "                # terminated (bool): whether a `terminal state` is reached (only True if the state is the Goal or a Hole!)\n",
        "                # truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied (eg. timelimit/agent physically going out of bounds)\n",
        "                # info (dict): contains auxiliary diagnostic information (might be helpful for debugging, though not used in this assignment)\n",
        "\n",
        "          done = truncated or terminated # we are done in either case\n",
        "\n",
        "          # visualization\n",
        "          total_rewards = pretty_print(i_episode, t, done, reward, total_rewards, debug=debug)\n",
        "          if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZtURZfCQJiN"
      },
      "source": [
        "## Conceptual Questions (ACTs 8-10)\n",
        "\n",
        "Notice that in your implementation above, an agent that follows the value-iteration policy performs much better than an agent that follows the random policy (you can run both variations by setting the `random_agent` variable to `True/False`).\n",
        "\n",
        "Please **briefly** answer the following questions (1-2 sentences per question suffices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0beR-B-QJiO"
      },
      "source": [
        "#### **ACT8:** Does the value-iteration policy *always* obtains reward=1? Explain why/why not.\n",
        "#### <span style='color:cyan'>  **Answer:** </span> The value iteration policy does not always obtain the reward because the reward is sparse in relation to the amount of states, so in some iterations of this case, the agent may fall into a hole."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gglBTJQJiO"
      },
      "source": [
        "#### **ACT9:** Will the random policy *always* obtain reward=0? Explain why/why not.\n",
        "#### <span style='color:cyan'>  **Answer:** </span> The random policy will not always obtain reward 0. As small as it is, there is a nonzero chance that the agent makes all of the right random moves and obtains the goal by pure chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdSUi0u3QJiO"
      },
      "source": [
        "#### **ACT10:** When averaged over many episodes, will the value-iteration policy always obtain a larger reward than the random policy? Explain why/why not (no need to prove it, just state the appropriate claim shown in lecture and breifly explain how it implies this statement).\n",
        "#### <span style='color:cyan'>  **Answer:** </span> Yes, it will always obtain a reward greater than that of a random policy because it is designed to converge at the 'optimal' policy, or the policy that maximizes the reward it can get."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}